{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7522318,"sourceType":"datasetVersion","datasetId":4381992}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:52:07.447365Z","iopub.execute_input":"2025-02-22T19:52:07.447553Z","iopub.status.idle":"2025-02-22T19:52:20.310354Z","shell.execute_reply.started":"2025-02-22T19:52:07.447534Z","shell.execute_reply":"2025-02-22T19:52:20.309670Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Base path\nbase_path = '/kaggle/input/fruits-dataset-for-classification'\n\n# Map directories to labels\nfruit_label_mapping = {\n    'fresh_peaches_done': 'fresh_peach',\n    'fresh_pomegranates_done': 'fresh_pomegranate',\n    'fresh_strawberries_done': 'fresh_strawberry',\n    'rotten_peaches_done': 'rotten_peach',\n    'rotten_pomegranates_done': 'rotten_pomegranate',\n    'rotten_strawberries_done': 'rotten_strawberry'\n}\n\n# Function to load & preprocess images\ndef load_and_preprocess_image(image_path):\n    image = Image.open(image_path).convert('RGB').resize((100, 100))\n    return np.array(image) / 255.0  # Normalize\n\n# Load images and labels\nall_images_data, all_labels = [], []\nfor dir_name, fruit_type in fruit_label_mapping.items():\n    directory = os.path.join(base_path, dir_name)\n    image_list = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(('.jpg', '.jpeg', '.png'))]\n    \n    if image_list:  # Ensure there are images\n        data = [load_and_preprocess_image(img) for img in image_list]\n        all_images_data.extend(data)\n        all_labels.extend([fruit_type] * len(data))\n\n# Convert to numpy arrays\nall_images_data, all_labels = np.array(all_images_data), np.array(all_labels)\n\n# Encode labels\nlabel_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(all_labels)\n\n# Train-test split\ntrain_images, val_images, train_labels, val_labels = train_test_split(\n    all_images_data, encoded_labels, test_size=0.2, random_state=42\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:52:20.311086Z","iopub.execute_input":"2025-02-22T19:52:20.311554Z","iopub.status.idle":"2025-02-22T19:52:30.230326Z","shell.execute_reply.started":"2025-02-22T19:52:20.311530Z","shell.execute_reply":"2025-02-22T19:52:30.229650Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load pre-trained VGG19 (without top layer)\nbase_model = VGG19(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\nfor layer in base_model.layers:\n    layer.trainable = False  # Freeze pre-trained layers\n\n# Add custom layers\nx = Flatten()(base_model.output)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.6)(x)\noutput = Dense(len(label_encoder.classes_), activation='softmax')(x)\n\n# Define & compile model\nmodel = Model(inputs=base_model.input, outputs=output)\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train model\nhistory = model.fit(train_images, train_labels, validation_data=(val_images, val_labels), epochs=20, batch_size=32)\n\n# Evaluate model\ntest_loss, test_acc = model.evaluate(val_images, val_labels)\nprint(f\"Validation Accuracy: {test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:52:30.231097Z","iopub.execute_input":"2025-02-22T19:52:30.231331Z","iopub.status.idle":"2025-02-22T19:53:08.902903Z","shell.execute_reply.started":"2025-02-22T19:52:30.231312Z","shell.execute_reply":"2025-02-22T19:53:08.902169Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 154ms/step - accuracy: 0.2001 - loss: 2.1177 - val_accuracy: 0.4441 - val_loss: 1.5324\nEpoch 2/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.3389 - loss: 1.6102 - val_accuracy: 0.6526 - val_loss: 1.2680\nEpoch 3/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.4461 - loss: 1.3981 - val_accuracy: 0.6677 - val_loss: 1.0928\nEpoch 4/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5467 - loss: 1.2199 - val_accuracy: 0.7009 - val_loss: 0.9884\nEpoch 5/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5553 - loss: 1.1496 - val_accuracy: 0.7160 - val_loss: 0.8880\nEpoch 6/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5955 - loss: 1.0647 - val_accuracy: 0.7281 - val_loss: 0.8292\nEpoch 7/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6439 - loss: 0.9911 - val_accuracy: 0.7251 - val_loss: 0.8011\nEpoch 8/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6524 - loss: 0.9172 - val_accuracy: 0.7221 - val_loss: 0.7735\nEpoch 9/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6944 - loss: 0.8537 - val_accuracy: 0.7311 - val_loss: 0.7335\nEpoch 10/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6719 - loss: 0.8703 - val_accuracy: 0.7311 - val_loss: 0.7100\nEpoch 11/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7033 - loss: 0.7612 - val_accuracy: 0.7492 - val_loss: 0.6796\nEpoch 12/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7290 - loss: 0.7253 - val_accuracy: 0.7553 - val_loss: 0.6715\nEpoch 13/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7435 - loss: 0.7058 - val_accuracy: 0.7432 - val_loss: 0.6484\nEpoch 14/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7459 - loss: 0.6844 - val_accuracy: 0.7583 - val_loss: 0.6239\nEpoch 15/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7559 - loss: 0.6432 - val_accuracy: 0.7795 - val_loss: 0.6262\nEpoch 16/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7613 - loss: 0.6020 - val_accuracy: 0.7825 - val_loss: 0.6040\nEpoch 17/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7981 - loss: 0.5622 - val_accuracy: 0.7946 - val_loss: 0.6030\nEpoch 18/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7820 - loss: 0.5725 - val_accuracy: 0.7915 - val_loss: 0.5816\nEpoch 19/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7831 - loss: 0.5782 - val_accuracy: 0.7492 - val_loss: 0.6298\nEpoch 20/20\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7893 - loss: 0.5563 - val_accuracy: 0.7915 - val_loss: 0.5688\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7878 - loss: 0.5747\nValidation Accuracy: 0.7915\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}